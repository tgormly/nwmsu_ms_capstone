{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analysis - Stint to Previous Stint Comparison\n",
    "\n",
    "Age, percent through career, teammates of same nationality, and teammates of same nationality vs previous stint will be used to predict whether the current stint will have a score higher or lower than the previous stint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Specify Features\n",
    "\n",
    "The player's first stint in the league is removed, as there is no previous stint to compare to, and these rows are unsuitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>age</th>\n",
       "      <th>percent_through_career</th>\n",
       "      <th>teammates_same_nationality</th>\n",
       "      <th>tsm_vs_prev_stint</th>\n",
       "      <th>stint_vs_prev_stint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.258373</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.612440</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1927</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.132132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1928</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.201201</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1929</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   age  percent_through_career  teammates_same_nationality  \\\n",
       "1  2010  23.0                0.258373                           3   \n",
       "2  2011  24.0                0.612440                           1   \n",
       "4  1927  27.0                0.132132                           0   \n",
       "5  1928  28.0                0.201201                           0   \n",
       "6  1929  29.0                0.333333                           0   \n",
       "\n",
       "   tsm_vs_prev_stint  stint_vs_prev_stint  \n",
       "1                0.0                  1.0  \n",
       "2               -1.0                  0.0  \n",
       "4                0.0                  0.0  \n",
       "5                0.0                  1.0  \n",
       "6                0.0                  1.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/Gold/main.csv')\n",
    "df = df[['year','age', 'percent_through_career', 'teammates_same_nationality', 'tsm_vs_prev_stint', 'stint_vs_prev_stint']]\n",
    "\n",
    "# Remove rows where 'stint_vs_prev_stint' is NaN\n",
    "df = df.dropna(subset=['stint_vs_prev_stint'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Result Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_columns = ('Run', 'Mean CV ROC AUC', 'Mean CV F1', 'Test ROC AUC', 'Test F1', 'Train ROC AUC', 'Train F1'\n",
    ")\n",
    "\n",
    "results_table = []\n",
    "\n",
    "results_table.append(table_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Analysis\n",
    "\n",
    "### Initial Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timgo\\OneDrive\\Desktop\\School\\Summer2024\\Capstone\\nwmsu_ms_capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Scores: [0.49259498 0.50817046 0.49270767 0.50706033 0.51579955]\n",
      "Mean Cross-Validation F1: 0.5032665990702172\n",
      "Cross-Validation ROC AUC Scores: [0.56377078 0.58463559 0.56151325 0.58414523 0.58684195]\n",
      "Mean Cross-Validation ROC AUC: 0.5761813594324071\n",
      "Test Set F1 Score: 0.5055455248903792\n",
      "Test Set ROC AUC: 0.5861509983237954\n",
      "Train Set F1 Score: 1.0\n",
      "Train Set ROC AUC: 1.0\n",
      "Feature Importances:\n",
      "                       Feature  Importance\n",
      "2      percent_through_career    0.427560\n",
      "0                        year    0.235091\n",
      "3  teammates_same_nationality    0.158197\n",
      "1                         age    0.130597\n",
      "4           tsm_vs_prev_stint    0.048555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "\n",
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_vs_prev_stint'])\n",
    "y = df['stint_vs_prev_stint']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model for classification\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Perform cross-validation and compute F1 and ROC AUC scores\n",
    "cv_f1_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=f1_scorer)\n",
    "cv_roc_auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=roc_auc_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
    "test_roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='binary')\n",
    "train_roc_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation F1 Scores:\", cv_f1_scores)\n",
    "print(\"Mean Cross-Validation F1:\", cv_f1_scores.mean())\n",
    "print(\"Cross-Validation ROC AUC Scores:\", cv_roc_auc_scores)\n",
    "print(\"Mean Cross-Validation ROC AUC:\", cv_roc_auc_scores.mean())\n",
    "\n",
    "print(\"Test Set F1 Score:\", test_f1)\n",
    "print(\"Test Set ROC AUC:\", test_roc_auc)\n",
    "print(\"Train Set F1 Score:\", train_f1)\n",
    "print(\"Train Set ROC AUC:\", train_roc_auc)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "# Append results to the results table\n",
    "results_table.append(('Initial Run', cv_roc_auc_scores.mean(), cv_f1_scores.mean(), test_roc_auc, test_f1, train_roc_auc, train_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ROC AUC of 0.5 is the equivalent of random guessing.  Our test ROC AUC of 0.589 is slightly better than random guessing, but is not excellent.  The F1 score of 0.506 is not incredible either. We would hope to see this closer to 0.7 or 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control - Only Age & Percent Through Career\n",
    "\n",
    "In order to see if the teammate features we have developed improve model performance, we create a control model that excludes these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Gold/main.csv')\n",
    "df = df[['year','age', 'percent_through_career', 'stint_vs_prev_stint']]\n",
    "# Remove rows where 'stint_vs_prev_stint' is NaN\n",
    "df = df.dropna(subset=['stint_vs_prev_stint'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timgo\\OneDrive\\Desktop\\School\\Summer2024\\Capstone\\nwmsu_ms_capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Scores: [0.50352653 0.49797445 0.49011125 0.50169075 0.51222571]\n",
      "Mean Cross-Validation F1: 0.5011057346521257\n",
      "Cross-Validation ROC AUC Scores: [0.54511965 0.56052111 0.5493164  0.55198245 0.57724271]\n",
      "Mean Cross-Validation ROC AUC: 0.5568364645002308\n",
      "Test Set F1 Score: 0.502773575390822\n",
      "Test Set ROC AUC: 0.5650047218926314\n",
      "Train Set F1 Score: 0.9998788759689923\n",
      "Train Set ROC AUC: 0.9999999741763581\n",
      "Feature Importances:\n",
      "                   Feature  Importance\n",
      "2  percent_through_career    0.660455\n",
      "0                    year    0.259147\n",
      "1                     age    0.080399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_vs_prev_stint'])\n",
    "y = df['stint_vs_prev_stint']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model for classification\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Perform cross-validation and compute F1 and ROC AUC scores\n",
    "cv_f1_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=f1_scorer)\n",
    "cv_roc_auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=roc_auc_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
    "test_roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='binary')\n",
    "train_roc_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation F1 Scores:\", cv_f1_scores)\n",
    "print(\"Mean Cross-Validation F1:\", cv_f1_scores.mean())\n",
    "print(\"Cross-Validation ROC AUC Scores:\", cv_roc_auc_scores)\n",
    "print(\"Mean Cross-Validation ROC AUC:\", cv_roc_auc_scores.mean())\n",
    "\n",
    "print(\"Test Set F1 Score:\", test_f1)\n",
    "print(\"Test Set ROC AUC:\", test_roc_auc)\n",
    "print(\"Train Set F1 Score:\", train_f1)\n",
    "print(\"Train Set ROC AUC:\", train_roc_auc)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "# Append results to the results table\n",
    "results_table.append(('Control', cv_roc_auc_scores.mean(), cv_f1_scores.mean(), test_roc_auc, test_f1, train_roc_auc, train_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Features\n",
    "\n",
    "Percent through career, year, and teammates_same_nationality were the top three features in the initial run with all features.  We will see if performance improves when these are the only features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Gold/main.csv')\n",
    "df = df[['year','percent_through_career', 'teammates_same_nationality', 'stint_vs_prev_stint']]\n",
    "# Remove rows where 'stint_vs_prev_stint' is NaN\n",
    "df = df.dropna(subset=['stint_vs_prev_stint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timgo\\OneDrive\\Desktop\\School\\Summer2024\\Capstone\\nwmsu_ms_capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Scores: [0.48271527 0.5070159  0.49071782 0.50770179 0.51731535]\n",
      "Mean Cross-Validation F1: 0.5010932277382446\n",
      "Cross-Validation ROC AUC Scores: [0.55271195 0.58085444 0.55003106 0.565417   0.57348263]\n",
      "Mean Cross-Validation ROC AUC: 0.564499417028769\n",
      "Test Set F1 Score: 0.5071196602548089\n",
      "Test Set ROC AUC: 0.5661619632973485\n",
      "Train Set F1 Score: 0.9998182809376703\n",
      "Train Set ROC AUC: 0.9999999031613431\n",
      "Feature Importances:\n",
      "                       Feature  Importance\n",
      "1      percent_through_career    0.668601\n",
      "0                        year    0.239432\n",
      "2  teammates_same_nationality    0.091967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_vs_prev_stint'])\n",
    "y = df['stint_vs_prev_stint']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model for classification\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Perform cross-validation and compute F1 and ROC AUC scores\n",
    "cv_f1_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=f1_scorer)\n",
    "cv_roc_auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=roc_auc_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
    "test_roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='binary')\n",
    "train_roc_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation F1 Scores:\", cv_f1_scores)\n",
    "print(\"Mean Cross-Validation F1:\", cv_f1_scores.mean())\n",
    "print(\"Cross-Validation ROC AUC Scores:\", cv_roc_auc_scores)\n",
    "print(\"Mean Cross-Validation ROC AUC:\", cv_roc_auc_scores.mean())\n",
    "\n",
    "print(\"Test Set F1 Score:\", test_f1)\n",
    "print(\"Test Set ROC AUC:\", test_roc_auc)\n",
    "print(\"Train Set F1 Score:\", train_f1)\n",
    "print(\"Train Set ROC AUC:\", train_roc_auc)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "# Append results to the results table\n",
    "results_table.append(('Reduced Features', cv_roc_auc_scores.mean(), cv_f1_scores.mean(), test_roc_auc, test_f1, train_roc_auc, train_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Experimentation\n",
    "\n",
    "To experiment with hyperparameters in an efficient way, a grid of parameters and a GridSearch are used to quickly experiment with a variety of hyperparameter combinations. The initial run had the best overall performance, so return to that feature set for hyperparameter experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Gold/main.csv')\n",
    "df = df[['year','age', 'percent_through_career', 'teammates_same_nationality', 'tsm_vs_prev_stint', 'stint_vs_prev_stint']]\n",
    "\n",
    "# Remove rows where 'stint_vs_prev_stint' is NaN\n",
    "df = df.dropna(subset=['stint_vs_prev_stint'])\n",
    "\n",
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_vs_prev_stint'])\n",
    "y = df['stint_vs_prev_stint']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best ROC AUC Score: 0.6106785975486931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with ROC AUC scoring\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='roc_auc')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best ROC AUC Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a model is executed with the hyper parameters that provide the best ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timgo\\OneDrive\\Desktop\\School\\Summer2024\\Capstone\\nwmsu_ms_capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Scores: [0.50813273 0.51759973 0.50542942 0.51554664 0.49722607]\n",
      "Mean Cross-Validation F1: 0.5087869180581406\n",
      "Cross-Validation ROC AUC Scores: [0.59637546 0.61841115 0.59766856 0.61806568 0.62732624]\n",
      "Mean Cross-Validation ROC AUC: 0.6115694169534902\n",
      "Test Set F1 Score: 0.5243804956035172\n",
      "Test Set ROC AUC: 0.6163039512193204\n",
      "Train Set F1 Score: 0.6162147933666754\n",
      "Train Set ROC AUC: 0.7524160211939793\n",
      "Feature Importances:\n",
      "                       Feature  Importance\n",
      "2      percent_through_career    0.481219\n",
      "0                        year    0.199137\n",
      "1                         age    0.154695\n",
      "3  teammates_same_nationality    0.125659\n",
      "4           tsm_vs_prev_stint    0.039291\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "\n",
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_vs_prev_stint'])\n",
    "y = df['stint_vs_prev_stint']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model for classification\n",
    "model = RandomForestClassifier(max_depth=10\n",
    "                               , min_samples_leaf=2\n",
    "                               , min_samples_split=2\n",
    "                               , random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Perform cross-validation and compute F1 and ROC AUC scores\n",
    "cv_f1_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=f1_scorer)\n",
    "cv_roc_auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=roc_auc_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
    "test_roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='binary')\n",
    "train_roc_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation F1 Scores:\", cv_f1_scores)\n",
    "print(\"Mean Cross-Validation F1:\", cv_f1_scores.mean())\n",
    "print(\"Cross-Validation ROC AUC Scores:\", cv_roc_auc_scores)\n",
    "print(\"Mean Cross-Validation ROC AUC:\", cv_roc_auc_scores.mean())\n",
    "\n",
    "print(\"Test Set F1 Score:\", test_f1)\n",
    "print(\"Test Set ROC AUC:\", test_roc_auc)\n",
    "print(\"Train Set F1 Score:\", train_f1)\n",
    "print(\"Train Set ROC AUC:\", train_roc_auc)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "# Append results to the results table\n",
    "results_table.append(('Hyperparameter Experimentation', cv_roc_auc_scores.mean(), cv_f1_scores.mean(), test_roc_auc, test_f1, train_roc_auc, train_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Mean CV ROC AUC</th>\n",
       "      <th>Mean CV F1</th>\n",
       "      <th>Test ROC AUC</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Train ROC AUC</th>\n",
       "      <th>Train F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Initial Run</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.5033</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.5055</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Control</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.5011</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reduced Features</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.5011</td>\n",
       "      <td>0.5662</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyperparameter Experimentation</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.5088</td>\n",
       "      <td>0.6163</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>0.7524</td>\n",
       "      <td>0.6162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Run  Mean CV ROC AUC  Mean CV F1  Test ROC AUC  \\\n",
       "0                     Initial Run           0.5762      0.5033        0.5862   \n",
       "1                         Control           0.5568      0.5011        0.5650   \n",
       "2                Reduced Features           0.5645      0.5011        0.5662   \n",
       "3  Hyperparameter Experimentation           0.6116      0.5088        0.6163   \n",
       "\n",
       "   Test F1  Train ROC AUC  Train F1  \n",
       "0   0.5055         1.0000    1.0000  \n",
       "1   0.5028         1.0000    0.9999  \n",
       "2   0.5071         1.0000    0.9998  \n",
       "3   0.5244         0.7524    0.6162  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the column titles\n",
    "columns = results_table[0]\n",
    "\n",
    "# Extract the data values and round them to 4 decimal places\n",
    "values = [\n",
    "    (row[0], *[round(val, 4) for val in row[1:]])\n",
    "    for row in results_table[1:]\n",
    "]\n",
    "\n",
    "# Convert the values into a DataFrame\n",
    "results_df = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The feature selection in the Control and Reduced Features model did not improve performance over the initial run which included all features. None of the models performed particularly well, but the initial run with hyperparameter adjustments greatly improved overfitting, with more reasonable training data scores, and a set of higher overall test scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
