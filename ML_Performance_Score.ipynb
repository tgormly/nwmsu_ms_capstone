{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analysis - Performance Score (Stint Score)\n",
    "\n",
    "In this notebook, we conduct machine learning analysis to predict a player's stint_score, henceforth referred to as their performance score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Specify Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/Gold/main.csv')\n",
    "df = df[['year','age', 'percent_through_career', 'teammates_same_nationality', 'tsm_vs_prev_stint', 'stint_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Result Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_columns = ('Run', 'Mean CV MSE', 'Mean CV R²', 'Test MSE', 'Test R²', 'Train MSE', 'Train R²'\n",
    ")\n",
    "\n",
    "results_table = []\n",
    "\n",
    "results_table.append(table_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Analysis\n",
    "\n",
    "Because no clear linear relationship was found between the number of teammates sharing a player's nationality and that player's performance, we use a Random Forest. As a non-linear ensemble model, Random Forest can be effective at predictions in situations similar to this one.\n",
    "\n",
    "### Initial Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE Scores: [-0.86602951 -0.88224342 -0.86929714 -0.8565786  -0.89277956]\n",
      "Mean Cross-Validation MSE: 0.8733856450066917\n",
      "Cross-Validation R² Scores: [0.13384674 0.12111081 0.14075823 0.13600072 0.1086928 ]\n",
      "Mean Cross-Validation R²: 0.12808185901750171\n",
      "Test Set Mean Squared Error: 0.8669143163951625\n",
      "Test Set R-Squared: 0.12673396557694883\n",
      "Train Set Mean Squared Error: 0.13087317173234508\n",
      "Train Set R-Squared: 0.8693643696643413\n",
      "Feature Importances:\n",
      "                       Feature  Importance\n",
      "2      percent_through_career    0.471434\n",
      "0                        year    0.200815\n",
      "3  teammates_same_nationality    0.151379\n",
      "1                         age    0.131113\n",
      "4           tsm_vs_prev_stint    0.045259\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "\n",
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_score'])\n",
    "y = df['stint_score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Perform cross-validation and compute MSE and R² scores\n",
    "cv_mse_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=mse_scorer)\n",
    "cv_r2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=r2_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation MSE Scores:\", cv_mse_scores)\n",
    "print(\"Mean Cross-Validation MSE:\", -cv_mse_scores.mean())  # Negated to get positive value\n",
    "print(\"Cross-Validation R² Scores:\", cv_r2_scores)\n",
    "print(\"Mean Cross-Validation R²:\", cv_r2_scores.mean())\n",
    "\n",
    "print(\"Test Set Mean Squared Error:\", test_mse)\n",
    "print(\"Test Set R-Squared:\", test_r2)\n",
    "print(\"Train Set Mean Squared Error:\", train_mse)\n",
    "print(\"Train Set R-Squared:\", train_r2)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "results_table.append(('Initial Run', -cv_mse_scores.mean(), cv_r2_scores.mean(), test_mse, test_r2, train_mse, train_r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control - Only Age & Percent Through Career\n",
    "\n",
    "In order to see if the teammate features we have developed improve model performance, we create a control model that excludes these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Gold/main.csv')\n",
    "df = df[['year','age', 'percent_through_career', 'stint_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE Scores: [-0.91115088 -0.91953607 -0.91723727 -0.90427235 -0.93437222]\n",
      "Mean Cross-Validation MSE: 0.9173137586124425\n",
      "Cross-Validation R² Scores: [0.08871892 0.08395994 0.09337263 0.08789379 0.06716873]\n",
      "Mean Cross-Validation R²: 0.08422280270169116\n",
      "Test Set Mean Squared Error: 0.9058013272112092\n",
      "Test Set R-Squared: 0.08756203695186404\n",
      "Train Set Mean Squared Error: 0.15841510851828053\n",
      "Train Set R-Squared: 0.8418724228805199\n",
      "Feature Importances:\n",
      "                   Feature  Importance\n",
      "2  percent_through_career    0.610152\n",
      "0                    year    0.236674\n",
      "1                     age    0.153173\n"
     ]
    }
   ],
   "source": [
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_score'])\n",
    "y = df['stint_score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Perform cross-validation and compute MSE and R² scores\n",
    "cv_mse_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=mse_scorer)\n",
    "cv_r2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=r2_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation MSE Scores:\", cv_mse_scores)\n",
    "print(\"Mean Cross-Validation MSE:\", -cv_mse_scores.mean())  # Negated to get positive value\n",
    "print(\"Cross-Validation R² Scores:\", cv_r2_scores)\n",
    "print(\"Mean Cross-Validation R²:\", cv_r2_scores.mean())\n",
    "\n",
    "print(\"Test Set Mean Squared Error:\", test_mse)\n",
    "print(\"Test Set R-Squared:\", test_r2)\n",
    "print(\"Train Set Mean Squared Error:\", train_mse)\n",
    "print(\"Train Set R-Squared:\", train_r2)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "results_table.append(('Control', -cv_mse_scores.mean(), cv_r2_scores.mean(), test_mse, test_r2, train_mse, train_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance without the teammate same nationality variables is worse on all metrics compared to the original model that includes these features, indicating that the teammate features are beneficial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Features\n",
    "\n",
    "Percent through career, year, and teammates_same_nationality were the top three features in the initial run with all features.  We will see if performance improves when these are the only features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove age and tsm_vs_prev_stint\n",
    "df = pd.read_csv('Data/Gold/main.csv')\n",
    "df = df[['year', 'percent_through_career', 'teammates_same_nationality', 'stint_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE Scores: [-0.92382674 -0.93002508 -0.90755884 -0.91913583 -0.95260342]\n",
      "Mean Cross-Validation MSE: 0.9266299836399853\n",
      "Cross-Validation R² Scores: [0.07604125 0.07351081 0.10293911 0.07290154 0.04896759]\n",
      "Mean Cross-Validation R²: 0.07487206092605529\n",
      "Test Set Mean Squared Error: 0.9268842159106914\n",
      "Test Set R-Squared: 0.06632467789504681\n",
      "Train Set Mean Squared Error: 0.15927714018388087\n",
      "Train Set R-Squared: 0.8410119558457994\n",
      "Feature Importances:\n",
      "                       Feature  Importance\n",
      "1      percent_through_career    0.601697\n",
      "0                        year    0.221464\n",
      "2  teammates_same_nationality    0.176839\n"
     ]
    }
   ],
   "source": [
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_score'])\n",
    "y = df['stint_score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Perform cross-validation and compute MSE and R² scores\n",
    "cv_mse_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=mse_scorer)\n",
    "cv_r2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=r2_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation MSE Scores:\", cv_mse_scores)\n",
    "print(\"Mean Cross-Validation MSE:\", -cv_mse_scores.mean())  # Negated to get positive value\n",
    "print(\"Cross-Validation R² Scores:\", cv_r2_scores)\n",
    "print(\"Mean Cross-Validation R²:\", cv_r2_scores.mean())\n",
    "\n",
    "print(\"Test Set Mean Squared Error:\", test_mse)\n",
    "print(\"Test Set R-Squared:\", test_r2)\n",
    "print(\"Train Set Mean Squared Error:\", train_mse)\n",
    "print(\"Train Set R-Squared:\", train_r2)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "results_table.append(('Reduced Features', -cv_mse_scores.mean(), cv_r2_scores.mean(), test_mse, test_r2, train_mse, train_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Experimentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance has gotten worse, so we will return to the original model. Additionally, all models so far show signs of overfitting, as train performance metrics are better than test metrics. To attempt to address this, we experiment with hyperparameters in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_score'])\n",
    "y = df['stint_score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To experiment with hyperparameters in an efficient way, a grid of parameters and a GridSearch are used to quickly experiment with a variety of hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5},\n",
       " np.float64(0.1774304182354913))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='r2')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ran again with the hyperparamters selected in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE Scores: [-0.82447198 -0.82187292 -0.81179235 -0.81205728 -0.8376697 ]\n",
      "Mean Cross-Validation MSE: 0.8215728469674051\n",
      "Cross-Validation R² Scores: [0.17541021 0.18125178 0.19759784 0.18090774 0.16371177]\n",
      "Mean Cross-Validation R²: 0.1797758673735608\n",
      "Test Set Mean Squared Error: 0.7995144078887128\n",
      "Test Set R-Squared: 0.19462770052719125\n",
      "Train Set Mean Squared Error: 0.7057429359881517\n",
      "Train Set R-Squared: 0.2955380229776705\n",
      "Feature Importances:\n",
      "                       Feature  Importance\n",
      "1      percent_through_career    0.724703\n",
      "0                        year    0.190125\n",
      "2  teammates_same_nationality    0.085171\n"
     ]
    }
   ],
   "source": [
    "# Split input features and target feature\n",
    "X = df.drop(columns=['stint_score'])\n",
    "y = df['stint_score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    max_depth=10, \n",
    "    min_samples_leaf=1, \n",
    "    min_samples_split=5\n",
    ")\n",
    "\n",
    "# Define the scoring metrics\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Perform cross-validation and compute MSE and R² scores\n",
    "cv_mse_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=mse_scorer)\n",
    "cv_r2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=r2_scorer)\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Cross-Validation MSE Scores:\", cv_mse_scores)\n",
    "print(\"Mean Cross-Validation MSE:\", -cv_mse_scores.mean())  # Negated to get positive value\n",
    "print(\"Cross-Validation R² Scores:\", cv_r2_scores)\n",
    "print(\"Mean Cross-Validation R²:\", cv_r2_scores.mean())\n",
    "\n",
    "print(\"Test Set Mean Squared Error:\", test_mse)\n",
    "print(\"Test Set R-Squared:\", test_r2)\n",
    "print(\"Train Set Mean Squared Error:\", train_mse)\n",
    "print(\"Train Set R-Squared:\", train_r2)\n",
    "print(\"Feature Importances:\\n\", feature_importances_df)\n",
    "\n",
    "results_table.append(('Hyperparameters Adjusted', -cv_mse_scores.mean(), cv_r2_scores.mean(), test_mse, test_r2, train_mse, train_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance has increased, and the signs of overfitting have improved as well. Performance on the training data specifically has decreased, but the model generalizes to unseen data much better after making hyperparameter adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Mean CV MSE</th>\n",
       "      <th>Mean CV R²</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test R²</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Train R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Initial Run</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>0.1281</td>\n",
       "      <td>0.8669</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.8694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Control</td>\n",
       "      <td>0.9173</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.1584</td>\n",
       "      <td>0.8419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reduced Features</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>0.9269</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.8410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyperparameters Adjusted</td>\n",
       "      <td>0.8216</td>\n",
       "      <td>0.1798</td>\n",
       "      <td>0.7995</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.7057</td>\n",
       "      <td>0.2955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Run  Mean CV MSE  Mean CV R²  Test MSE  Test R²  \\\n",
       "0               Initial Run       0.8734      0.1281    0.8669   0.1267   \n",
       "1                   Control       0.9173      0.0842    0.9058   0.0876   \n",
       "2          Reduced Features       0.9266      0.0749    0.9269   0.0663   \n",
       "3  Hyperparameters Adjusted       0.8216      0.1798    0.7995   0.1946   \n",
       "\n",
       "   Train MSE  Train R²  \n",
       "0     0.1309    0.8694  \n",
       "1     0.1584    0.8419  \n",
       "2     0.1593    0.8410  \n",
       "3     0.7057    0.2955  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the column titles\n",
    "columns = results_table[0]\n",
    "\n",
    "# Extract the data values and round them to 4 decimal places\n",
    "values = [\n",
    "    (row[0], *[round(val, 4) for val in row[1:]])\n",
    "    for row in results_table[1:]\n",
    "]\n",
    "\n",
    "# Convert the values into a DataFrame\n",
    "results_df = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the model runs had particularly impressive results. Only a small amount of the variation in the target variable can be attributed to the input features in our best case. That said, we found that the teammate variables can be beneficial to predicting a player's performance score. When these variables were removed, model performance decreased. In the initial model run, the R² of 0.128 is not particularly impressive, but it is markedly higher than the R² score when only age, year, and percent_through_career were considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
